{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d959ee",
   "metadata": {},
   "source": [
    "# Pandas Tutorial: Data Analysis and Manipulation with Python\n",
    "\n",
    "Pandas (Python Data Analysis Library) is a powerful, open-source data analysis and manipulation library built on top of NumPy. It provides:\n",
    "- High-performance data structures (DataFrame and Series)\n",
    "- Data cleaning and preparation tools\n",
    "- Data input/output capabilities (CSV, Excel, JSON, SQL, etc.)\n",
    "- Data aggregation and transformation functions\n",
    "- Time series functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080ce00",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's check our Python version and ensure Pandas is properly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50093eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b534e",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "If Pandas is not installed, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd204de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37754040",
   "metadata": {},
   "source": [
    "## 1. Importing Pandas and Essential Libraries\n",
    "\n",
    "The standard convention is to import Pandas with the alias `pd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check versions\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4665a298",
   "metadata": {},
   "source": [
    "## 2. Pandas Data Structures\n",
    "\n",
    "Pandas has two main data structures:\n",
    "- **Series**: 1-dimensional labeled array (like a column)\n",
    "- **DataFrame**: 2-dimensional labeled data structure (like a table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aadc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Series\n",
    "print(\"=== PANDAS SERIES ===\")\n",
    "series_from_list = pd.Series([1, 2, 3, 4, 5])\n",
    "print(\"Series from list:\")\n",
    "print(series_from_list)\n",
    "print(f\"Type: {type(series_from_list)}\")\n",
    "print(f\"Shape: {series_from_list.shape}\")\n",
    "print(f\"Size: {series_from_list.size}\")\n",
    "\n",
    "# Series with custom index\n",
    "series_with_index = pd.Series([10, 20, 30], index=['a', 'b', 'c'])\n",
    "print(\"\\nSeries with custom index:\")\n",
    "print(series_with_index)\n",
    "\n",
    "# Series from dictionary\n",
    "series_from_dict = pd.Series({'apple': 5, 'banana': 3, 'orange': 8})\n",
    "print(\"\\nSeries from dictionary:\")\n",
    "print(series_from_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrames\n",
    "print(\"=== PANDAS DATAFRAME ===\")\n",
    "\n",
    "# DataFrame from dictionary\n",
    "data_dict = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'Age': [25, 30, 35, 28],\n",
    "    'City': ['New York', 'London', 'Tokyo', 'Paris'],\n",
    "    'Salary': [50000, 60000, 70000, 55000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "print(\"DataFrame from dictionary:\")\n",
    "print(df)\n",
    "print(f\"\\nType: {type(df)}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Size: {df.size}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Index: {list(df.index)}\")\n",
    "\n",
    "# DataFrame info\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d79b6",
   "metadata": {},
   "source": [
    "## 3. Data Selection and Indexing\n",
    "\n",
    "Pandas provides several ways to select and access data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column selection\n",
    "print(\"=== COLUMN SELECTION ===\")\n",
    "print(\"Single column (Name):\")\n",
    "print(df['Name'])\n",
    "print(f\"Type: {type(df['Name'])}\")\n",
    "\n",
    "print(\"\\nMultiple columns:\")\n",
    "print(df[['Name', 'Age']])\n",
    "\n",
    "# Row selection using .loc and .iloc\n",
    "print(\"\\n=== ROW SELECTION ===\")\n",
    "print(\"First row using .iloc:\")\n",
    "print(df.iloc[0])\n",
    "\n",
    "print(\"\\nFirst two rows using .iloc:\")\n",
    "print(df.iloc[0:2])\n",
    "\n",
    "print(\"\\nRows using .loc (label-based):\")\n",
    "print(df.loc[0:1])\n",
    "\n",
    "# Boolean indexing\n",
    "print(\"\\n=== BOOLEAN INDEXING ===\")\n",
    "print(\"People older than 30:\")\n",
    "print(df[df['Age'] > 30])\n",
    "\n",
    "print(\"\\nPeople from specific cities:\")\n",
    "print(df[df['City'].isin(['London', 'Tokyo'])])\n",
    "\n",
    "# Combined conditions\n",
    "print(\"\\nPeople older than 25 AND from New York or Paris:\")\n",
    "print(df[(df['Age'] > 25) & (df['City'].isin(['New York', 'Paris']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b2be3",
   "metadata": {},
   "source": [
    "## 4. Data Manipulation and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns\n",
    "print(\"=== ADDING COLUMNS ===\")\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Add calculated column\n",
    "df_copy['Salary_K'] = df_copy['Salary'] / 1000\n",
    "print(\"Added Salary in thousands:\")\n",
    "print(df_copy)\n",
    "\n",
    "# Add conditional column\n",
    "df_copy['Age_Group'] = df_copy['Age'].apply(lambda x: 'Young' if x < 30 else 'Adult')\n",
    "print(\"\\nAdded Age Group:\")\n",
    "print(df_copy)\n",
    "\n",
    "# Modifying existing columns\n",
    "print(\"\\n=== MODIFYING COLUMNS ===\")\n",
    "df_copy['Name'] = df_copy['Name'].str.upper()\n",
    "print(\"Names in uppercase:\")\n",
    "print(df_copy['Name'])\n",
    "\n",
    "# Dropping columns\n",
    "print(\"\\n=== DROPPING COLUMNS ===\")\n",
    "df_dropped = df_copy.drop(['Age_Group'], axis=1)\n",
    "print(\"After dropping Age_Group column:\")\n",
    "print(df_dropped.columns.tolist())\n",
    "\n",
    "# Sorting\n",
    "print(\"\\n=== SORTING ===\")\n",
    "print(\"Sorted by Age:\")\n",
    "print(df.sort_values('Age'))\n",
    "\n",
    "print(\"\\nSorted by multiple columns:\")\n",
    "print(df.sort_values(['Age', 'Salary'], ascending=[True, False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef8dfd",
   "metadata": {},
   "source": [
    "## 5. Data Aggregation and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1958799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger dataset for grouping examples\n",
    "np.random.seed(42)\n",
    "large_data = {\n",
    "    'Department': np.random.choice(['IT', 'HR', 'Finance', 'Marketing'], 100),\n",
    "    'Salary': np.random.randint(40000, 100000, 100),\n",
    "    'Age': np.random.randint(22, 60, 100),\n",
    "    'Experience': np.random.randint(0, 20, 100)\n",
    "}\n",
    "\n",
    "df_large = pd.DataFrame(large_data)\n",
    "print(\"Large dataset sample:\")\n",
    "print(df_large.head())\n",
    "\n",
    "print(f\"\\nDataset shape: {df_large.shape}\")\n",
    "\n",
    "# Basic aggregations\n",
    "print(\"\\n=== BASIC AGGREGATIONS ===\")\n",
    "print(f\"Mean salary: ${df_large['Salary'].mean():.2f}\")\n",
    "print(f\"Median salary: ${df_large['Salary'].median():.2f}\")\n",
    "print(f\"Standard deviation: ${df_large['Salary'].std():.2f}\")\n",
    "print(f\"Min salary: ${df_large['Salary'].min()}\")\n",
    "print(f\"Max salary: ${df_large['Salary'].max()}\")\n",
    "\n",
    "# Describe method\n",
    "print(\"\\n=== STATISTICAL SUMMARY ===\")\n",
    "print(df_large.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy operations\n",
    "print(\"=== GROUPBY OPERATIONS ===\")\n",
    "\n",
    "# Group by department\n",
    "dept_groups = df_large.groupby('Department')\n",
    "\n",
    "print(\"Average salary by department:\")\n",
    "print(dept_groups['Salary'].mean().sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nMultiple aggregations by department:\")\n",
    "dept_agg = dept_groups.agg({\n",
    "    'Salary': ['mean', 'min', 'max', 'count'],\n",
    "    'Age': ['mean', 'min', 'max'],\n",
    "    'Experience': ['mean', 'min', 'max']\n",
    "})\n",
    "print(dept_agg)\n",
    "\n",
    "# Value counts\n",
    "print(\"\\n=== VALUE COUNTS ===\")\n",
    "print(\"Department distribution:\")\n",
    "print(df_large['Department'].value_counts())\n",
    "\n",
    "# Cross-tabulation\n",
    "print(\"\\n=== CROSS-TABULATION ===\")\n",
    "# Create age groups for cross-tab\n",
    "df_large['Age_Group'] = pd.cut(df_large['Age'], bins=[20, 30, 40, 50, 60], labels=['20-30', '30-40', '40-50', '50-60'])\n",
    "crosstab = pd.crosstab(df_large['Department'], df_large['Age_Group'])\n",
    "print(\"Department vs Age Group cross-tabulation:\")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f420523d",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning and Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with missing values\n",
    "messy_data = {\n",
    "    'Name': ['Alice', 'Bob', None, 'Diana', 'Eve'],\n",
    "    'Age': [25, None, 35, 28, 31],\n",
    "    'Salary': [50000, 60000, None, 55000, None],\n",
    "    'City': ['New York', 'London', 'Tokyo', None, 'Berlin']\n",
    "}\n",
    "\n",
    "df_messy = pd.DataFrame(messy_data)\n",
    "print(\"Dataset with missing values:\")\n",
    "print(df_messy)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n=== MISSING VALUES ANALYSIS ===\")\n",
    "print(\"Missing values count:\")\n",
    "print(df_messy.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values percentage:\")\n",
    "print((df_messy.isnull().sum() / len(df_messy)) * 100)\n",
    "\n",
    "print(\"\\nRows with any missing values:\")\n",
    "print(df_messy[df_messy.isnull().any(axis=1)])\n",
    "\n",
    "# Handling missing values\n",
    "print(\"\\n=== HANDLING MISSING VALUES ===\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "print(\"After dropping rows with any missing values:\")\n",
    "print(df_messy.dropna())\n",
    "\n",
    "# Drop rows with all missing values\n",
    "print(\"\\nAfter dropping rows with all missing values:\")\n",
    "print(df_messy.dropna(how='all'))\n",
    "\n",
    "# Fill missing values\n",
    "print(\"\\nFill missing values with defaults:\")\n",
    "df_filled = df_messy.fillna({\n",
    "    'Name': 'Unknown',\n",
    "    'Age': df_messy['Age'].mean(),\n",
    "    'Salary': df_messy['Salary'].median(),\n",
    "    'City': 'Unknown'\n",
    "})\n",
    "print(df_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46277b2",
   "metadata": {},
   "source": [
    "## 7. Data Input/Output Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe62635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to various formats\n",
    "print(\"=== SAVING DATA ===\")\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('sample_data.csv', index=False)\n",
    "print(\"Data saved to CSV file\")\n",
    "\n",
    "# Save to JSON\n",
    "df.to_json('sample_data.json', orient='records', indent=2)\n",
    "print(\"Data saved to JSON file\")\n",
    "\n",
    "# Save to Excel (requires openpyxl or xlsxwriter)\n",
    "try:\n",
    "    df.to_excel('sample_data.xlsx', index=False)\n",
    "    print(\"Data saved to Excel file\")\n",
    "except ImportError:\n",
    "    print(\"Excel export requires openpyxl: pip install openpyxl\")\n",
    "\n",
    "# Reading data back\n",
    "print(\"\\n=== READING DATA ===\")\n",
    "\n",
    "# Read from CSV\n",
    "df_from_csv = pd.read_csv('sample_data.csv')\n",
    "print(\"Data read from CSV:\")\n",
    "print(df_from_csv)\n",
    "\n",
    "# Read from JSON\n",
    "df_from_json = pd.read_json('sample_data.json')\n",
    "print(\"\\nData read from JSON:\")\n",
    "print(df_from_json)\n",
    "\n",
    "# Creating sample data from various sources\n",
    "print(\"\\n=== CREATING DATA FROM DIFFERENT SOURCES ===\")\n",
    "\n",
    "# From lists of lists\n",
    "data_lists = [\n",
    "    ['Product A', 100, 25.99],\n",
    "    ['Product B', 50, 15.50],\n",
    "    ['Product C', 75, 32.00]\n",
    "]\n",
    "\n",
    "df_products = pd.DataFrame(data_lists, columns=['Product', 'Quantity', 'Price'])\n",
    "print(\"DataFrame from lists:\")\n",
    "print(df_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd2b40",
   "metadata": {},
   "source": [
    "## 8. Data Merging and Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38103258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames for merging\n",
    "employees = pd.DataFrame({\n",
    "    'emp_id': [1, 2, 3, 4],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'department': ['IT', 'HR', 'IT', 'Finance']\n",
    "})\n",
    "\n",
    "salaries = pd.DataFrame({\n",
    "    'emp_id': [1, 2, 3, 5],\n",
    "    'salary': [50000, 45000, 60000, 55000],\n",
    "    'bonus': [5000, 3000, 8000, 4000]\n",
    "})\n",
    "\n",
    "print(\"Employees DataFrame:\")\n",
    "print(employees)\n",
    "print(\"\\nSalaries DataFrame:\")\n",
    "print(salaries)\n",
    "\n",
    "# Different types of joins\n",
    "print(\"\\n=== MERGE OPERATIONS ===\")\n",
    "\n",
    "# Inner join (default)\n",
    "inner_join = pd.merge(employees, salaries, on='emp_id', how='inner')\n",
    "print(\"Inner join (only matching records):\")\n",
    "print(inner_join)\n",
    "\n",
    "# Left join\n",
    "left_join = pd.merge(employees, salaries, on='emp_id', how='left')\n",
    "print(\"\\nLeft join (all employees):\")\n",
    "print(left_join)\n",
    "\n",
    "# Right join\n",
    "right_join = pd.merge(employees, salaries, on='emp_id', how='right')\n",
    "print(\"\\nRight join (all salary records):\")\n",
    "print(right_join)\n",
    "\n",
    "# Outer join\n",
    "outer_join = pd.merge(employees, salaries, on='emp_id', how='outer')\n",
    "print(\"\\nOuter join (all records):\")\n",
    "print(outer_join)\n",
    "\n",
    "# Concatenation\n",
    "print(\"\\n=== CONCATENATION ===\")\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "\n",
    "print(\"Vertical concatenation:\")\n",
    "print(pd.concat([df1, df2], axis=0, ignore_index=True))\n",
    "\n",
    "print(\"\\nHorizontal concatenation:\")\n",
    "print(pd.concat([df1, df2], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bacbf",
   "metadata": {},
   "source": [
    "## 9. Time Series Data\n",
    "\n",
    "Pandas has excellent support for working with dates and time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6cad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with dates and times\n",
    "print(\"=== DATE AND TIME OPERATIONS ===\")\n",
    "\n",
    "# Create date range\n",
    "dates = pd.date_range('2024-01-01', periods=10, freq='D')\n",
    "print(\"Date range:\")\n",
    "print(dates)\n",
    "\n",
    "# Create time series data\n",
    "np.random.seed(42)\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'sales': np.random.randint(100, 1000, 10),\n",
    "    'temperature': np.random.normal(20, 5, 10)\n",
    "})\n",
    "\n",
    "print(\"\\nTime series data:\")\n",
    "print(ts_data)\n",
    "\n",
    "# Set date as index\n",
    "ts_data.set_index('date', inplace=True)\n",
    "print(\"\\nWith date as index:\")\n",
    "print(ts_data.head())\n",
    "\n",
    "# Date-based operations\n",
    "print(\"\\n=== DATE-BASED OPERATIONS ===\")\n",
    "print(\"Data for a specific date:\")\n",
    "print(ts_data.loc['2024-01-05'])\n",
    "\n",
    "print(\"\\nData for a date range:\")\n",
    "print(ts_data.loc['2024-01-03':'2024-01-07'])\n",
    "\n",
    "# Resampling (aggregating by time periods)\n",
    "print(\"\\n=== RESAMPLING ===\")\n",
    "# Create more data for resampling example\n",
    "extended_dates = pd.date_range('2024-01-01', periods=100, freq='D')\n",
    "extended_data = pd.DataFrame({\n",
    "    'date': extended_dates,\n",
    "    'value': np.random.randint(50, 200, 100)\n",
    "})\n",
    "extended_data.set_index('date', inplace=True)\n",
    "\n",
    "print(\"Weekly average:\")\n",
    "print(extended_data.resample('W').mean().head())\n",
    "\n",
    "print(\"\\nMonthly sum:\")\n",
    "print(extended_data.resample('M').sum())\n",
    "\n",
    "# Extract date components\n",
    "extended_data['year'] = extended_data.index.year\n",
    "extended_data['month'] = extended_data.index.month\n",
    "extended_data['day_of_week'] = extended_data.index.dayofweek\n",
    "\n",
    "print(\"\\nData with extracted date components:\")\n",
    "print(extended_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de4201",
   "metadata": {},
   "source": [
    "## 10. Practical Example: Sales Data Analysis\n",
    "\n",
    "Let's apply pandas to a real-world scenario - analyzing sales data for a retail store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915fcb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive sales dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create date range for 1 year\n",
    "dates = pd.date_range('2024-01-01', '2024-12-31', freq='D')\n",
    "n_days = len(dates)\n",
    "\n",
    "# Product categories and names\n",
    "categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports']\n",
    "products = {\n",
    "    'Electronics': ['Laptop', 'Phone', 'Tablet', 'Camera'],\n",
    "    'Clothing': ['Shirt', 'Pants', 'Dress', 'Shoes'],\n",
    "    'Books': ['Fiction', 'Non-Fiction', 'Textbook', 'Comic'],\n",
    "    'Home': ['Furniture', 'Kitchen', 'Decor', 'Garden'],\n",
    "    'Sports': ['Equipment', 'Apparel', 'Shoes', 'Accessories']\n",
    "}\n",
    "\n",
    "# Generate sales data\n",
    "sales_data = []\n",
    "for date in dates:\n",
    "    # Generate 5-15 transactions per day\n",
    "    n_transactions = np.random.randint(5, 16)\n",
    "    \n",
    "    for _ in range(n_transactions):\n",
    "        category = np.random.choice(categories)\n",
    "        product = np.random.choice(products[category])\n",
    "        quantity = np.random.randint(1, 6)\n",
    "        base_price = np.random.uniform(10, 500)\n",
    "        \n",
    "        # Add seasonal variation\n",
    "        month = date.month\n",
    "        if month in [11, 12]:  # Holiday season\n",
    "            base_price *= 1.2\n",
    "        elif month in [6, 7, 8]:  # Summer\n",
    "            if category == 'Sports':\n",
    "                base_price *= 1.15\n",
    "        \n",
    "        sales_data.append({\n",
    "            'date': date,\n",
    "            'category': category,\n",
    "            'product': product,\n",
    "            'quantity': quantity,\n",
    "            'unit_price': round(base_price, 2),\n",
    "            'total_amount': round(base_price * quantity, 2)\n",
    "        })\n",
    "\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "print(\"=== SALES DATA ANALYSIS ===\")\n",
    "print(f\"Total records: {len(sales_df)}\")\n",
    "print(f\"Date range: {sales_df['date'].min()} to {sales_df['date'].max()}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(sales_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c68938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive sales analysis\n",
    "print(\"=== BUSINESS INSIGHTS ===\")\n",
    "\n",
    "# Overall performance\n",
    "total_revenue = sales_df['total_amount'].sum()\n",
    "total_transactions = len(sales_df)\n",
    "avg_transaction_value = sales_df['total_amount'].mean()\n",
    "\n",
    "print(f\"Total Revenue: ${total_revenue:,.2f}\")\n",
    "print(f\"Total Transactions: {total_transactions:,}\")\n",
    "print(f\"Average Transaction Value: ${avg_transaction_value:.2f}\")\n",
    "\n",
    "# Category performance\n",
    "print(\"\\n=== CATEGORY ANALYSIS ===\")\n",
    "category_performance = sales_df.groupby('category').agg({\n",
    "    'total_amount': ['sum', 'count', 'mean'],\n",
    "    'quantity': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "category_performance.columns = ['Revenue', 'Transactions', 'Avg_Value', 'Units_Sold']\n",
    "category_performance = category_performance.sort_values('Revenue', ascending=False)\n",
    "print(category_performance)\n",
    "\n",
    "# Monthly trends\n",
    "print(\"\\n=== MONTHLY TRENDS ===\")\n",
    "sales_df['month'] = sales_df['date'].dt.month\n",
    "sales_df['month_name'] = sales_df['date'].dt.month_name()\n",
    "\n",
    "monthly_sales = sales_df.groupby(['month', 'month_name'])['total_amount'].sum().reset_index()\n",
    "monthly_sales = monthly_sales.sort_values('month')\n",
    "print(\"Monthly Revenue:\")\n",
    "for _, row in monthly_sales.iterrows():\n",
    "    print(f\"{row['month_name']}: ${row['total_amount']:,.2f}\")\n",
    "\n",
    "# Top products\n",
    "print(\"\\n=== TOP PRODUCTS ===\")\n",
    "product_performance = sales_df.groupby(['category', 'product']).agg({\n",
    "    'total_amount': 'sum',\n",
    "    'quantity': 'sum'\n",
    "}).sort_values('total_amount', ascending=False)\n",
    "\n",
    "print(\"Top 10 products by revenue:\")\n",
    "print(product_performance.head(10))\n",
    "\n",
    "# Day of week analysis\n",
    "print(\"\\n=== DAY OF WEEK ANALYSIS ===\")\n",
    "sales_df['day_of_week'] = sales_df['date'].dt.day_name()\n",
    "daily_pattern = sales_df.groupby('day_of_week')['total_amount'].agg(['sum', 'count', 'mean']).round(2)\n",
    "daily_pattern.columns = ['Total_Revenue', 'Transactions', 'Avg_Value']\n",
    "\n",
    "# Reorder by actual day order\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_pattern = daily_pattern.reindex(day_order)\n",
    "print(daily_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533c877",
   "metadata": {},
   "source": [
    "## 11. Advanced Pandas Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot tables and reshaping\n",
    "print(\"=== PIVOT TABLES ===\")\n",
    "\n",
    "# Create pivot table\n",
    "pivot_table = sales_df.pivot_table(\n",
    "    values='total_amount',\n",
    "    index='category',\n",
    "    columns='month_name',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ").round(2)\n",
    "\n",
    "print(\"Revenue by Category and Month:\")\n",
    "print(pivot_table)\n",
    "\n",
    "# Melt (unpivot) operation\n",
    "print(\"\\n=== MELTING DATA ===\")\n",
    "sample_wide = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob'],\n",
    "    'Math': [90, 85],\n",
    "    'Science': [88, 92],\n",
    "    'English': [95, 89]\n",
    "})\n",
    "\n",
    "print(\"Wide format:\")\n",
    "print(sample_wide)\n",
    "\n",
    "melted = pd.melt(sample_wide, id_vars=['Name'], var_name='Subject', value_name='Score')\n",
    "print(\"\\nMelted (long) format:\")\n",
    "print(melted)\n",
    "\n",
    "# Apply and lambda functions\n",
    "print(\"\\n=== APPLY FUNCTIONS ===\")\n",
    "sample_df = pd.DataFrame({\n",
    "    'name': ['alice smith', 'bob jones', 'charlie brown'],\n",
    "    'age': [25, 30, 35],\n",
    "    'salary': [50000, 60000, 70000]\n",
    "})\n",
    "\n",
    "# Apply lambda to single column\n",
    "sample_df['name_title'] = sample_df['name'].apply(lambda x: x.title())\n",
    "print(\"Applied title case to names:\")\n",
    "print(sample_df)\n",
    "\n",
    "# Apply function to multiple columns\n",
    "def categorize_age(age):\n",
    "    if age < 30:\n",
    "        return 'Young'\n",
    "    elif age < 40:\n",
    "        return 'Middle'\n",
    "    else:\n",
    "        return 'Senior'\n",
    "\n",
    "sample_df['age_category'] = sample_df['age'].apply(categorize_age)\n",
    "print(\"\\nAge categorization:\")\n",
    "print(sample_df)\n",
    "\n",
    "# String operations\n",
    "print(\"\\n=== STRING OPERATIONS ===\")\n",
    "text_data = pd.Series(['Hello World', 'pandas is great', 'Data Analysis', 'Python Programming'])\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text_data)\n",
    "\n",
    "print(\"\\nString operations:\")\n",
    "print(\"Uppercase:\", text_data.str.upper())\n",
    "print(\"Length:\", text_data.str.len())\n",
    "print(\"Contains 'Data':\", text_data.str.contains('Data'))\n",
    "print(\"Split on space:\", text_data.str.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d32fc7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered the essential concepts of Pandas:\n",
    "\n",
    "1. **Data Structures**: Series and DataFrame creation and properties\n",
    "2. **Data Selection**: Various methods to access and filter data\n",
    "3. **Data Manipulation**: Adding, modifying, and transforming data\n",
    "4. **Aggregation and Grouping**: Statistical analysis and groupby operations\n",
    "5. **Data Cleaning**: Handling missing values and data quality issues\n",
    "6. **Input/Output**: Reading from and writing to various file formats\n",
    "7. **Merging and Joining**: Combining multiple datasets\n",
    "8. **Time Series**: Working with dates and time-based data\n",
    "9. **Practical Applications**: Real-world sales data analysis\n",
    "10. **Advanced Techniques**: Pivot tables, apply functions, and string operations\n",
    "\n",
    "### Key Pandas Advantages:\n",
    "- **Intuitive Data Structures**: DataFrame and Series for structured data\n",
    "- **Powerful Data Manipulation**: Rich set of functions for data transformation\n",
    "- **Excellent I/O Capabilities**: Support for multiple file formats\n",
    "- **Built-in Data Analysis**: Statistical functions and aggregations\n",
    "- **Time Series Support**: Comprehensive date/time functionality\n",
    "\n",
    "### Next Steps:\n",
    "- Explore data visualization with matplotlib and seaborn\n",
    "- Learn advanced statistical analysis with scipy\n",
    "- Study machine learning with scikit-learn\n",
    "- Practice with real datasets from various domains\n",
    "\n",
    "### Resources:\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/)\n",
    "- [10 Minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html)\n",
    "- [Pandas Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
